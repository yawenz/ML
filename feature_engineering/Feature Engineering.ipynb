{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----ipass=0----------\n",
      "Label set: ['False', 'True']\n",
      "0.74526600541\n",
      "-----ipass=1----------\n",
      "Label set: ['False', 'True']\n",
      "0.734445446348\n",
      "-----ipass=2----------\n",
      "Label set: ['False', 'True']\n",
      "0.740982867448\n",
      "-----ipass=3----------\n",
      "Label set: ['True', 'False']\n",
      "0.738052299369\n",
      "-----ipass=4----------\n",
      "Label set: ['True', 'False']\n",
      "0.73219116321\n",
      "0.738187556357\n"
     ]
    }
   ],
   "source": [
    "from csv import DictReader, DictWriter\n",
    "\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import random\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import FunctionTransformer, Normalizer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "kTARGET_FIELD = 'spoiler'\n",
    "kTEXT_FIELD = 'sentence'\n",
    "kTROPE_FIELD = 'trope'\n",
    "kPAGE_FIELD = 'page'\n",
    "\n",
    "\n",
    "class Featurizer:\n",
    "    def __init__(self):\n",
    "        self.vectorizer = FeatureUnion( \n",
    "        [       \n",
    "                # Feature 1: word frequency\n",
    "                ('bag of words', \n",
    "                  Pipeline([('extract_field', FunctionTransformer(lambda x: x[0], validate = False)),\n",
    "                            ('count', TfidfVectorizer(stop_words = 'english'))])),            \n",
    "                # Feature 2: whether or not a page word appear in sentence\n",
    "                ('page in words',\n",
    "                  Pipeline([('extract_field', FunctionTransformer(lambda x: [x[0], x[2]], validate = False)), \n",
    "                            ('page', PageTransformer())])),\n",
    "                # Feature 3: appearance of some tropes, indicating spoiler\n",
    "                ('type of trope', \n",
    "                  Pipeline([('extract_field', FunctionTransformer(lambda x: x[1], validate = False)),\n",
    "                            ('count', CountVectorizer())]))                \n",
    "        \n",
    "            ])\n",
    "    \n",
    "    def train_feature(self, examples):\n",
    "        return self.vectorizer.fit_transform(examples)\n",
    "        \n",
    "    def test_feature(self, examples):\n",
    "        return self.vectorizer.transform(examples)\n",
    "\n",
    "\n",
    "# Figure out the frequency of page words appearing in the sentence, like \"Nikita\"\n",
    "class PageTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, examples):\n",
    "        # return self and nothing else \n",
    "        return self\n",
    "    \n",
    "    def transform(self, examples):\n",
    "        \n",
    "        import numpy as np \n",
    "        from scipy.sparse import csr_matrix\n",
    "            \n",
    "        # Initiaize matrix \n",
    "        X = np.zeros((len(examples[0]), 1))\n",
    "        \n",
    "        # Loop over examples and count letters \n",
    "        for ii, x in enumerate(examples[0]):            \n",
    "            sentence_word = nltk.word_tokenize(x)\n",
    "            # original: DreamHigh, using re to separate them\n",
    "            page = re.sub(r'([A-Z])', r' \\1', examples[1][ii])   \n",
    "            # get tokenizer of the sentence\n",
    "            page_word = nltk.word_tokenize(page)\n",
    "            # get the common words between sentence and page\n",
    "            common_word = set(sentence_word).intersection(page_word)\n",
    "            \n",
    "            # remove \"The\" which is common in page from common_word\n",
    "            if 'The' in common_word:\n",
    "                common_word.remove('The')\n",
    "                \n",
    "            X[ii,:] = len(common_word)\n",
    "        \n",
    "        # do normalization\n",
    "        X = preprocessing.normalize(X, norm='l2')\n",
    "        return csr_matrix(X)     \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Cast to list to keep it all in memory\n",
    "    train = list(DictReader(open(\"/Users/yawen/Desktop/CSCI 5622 Machine Learning/Homework 6/feature_engineering/data/spoilers/train.csv\", 'r')))\n",
    "    test = list(DictReader(open(\"/Users/yawen/Desktop/CSCI 5622 Machine Learning/Homework 6/feature_engineering/data/spoilers/test.csv\", 'r')))\n",
    "\n",
    "    # CrossValidation: add shuffle\n",
    "    npass=5\n",
    "    train_ratio=0.7 #test_ratio=0.3\n",
    "    ntrain=int(len(train)*train_ratio)\n",
    "    ntest=len(train)-ntrain\n",
    "    scores=np.zeros(npass)\n",
    "    \n",
    "    for ipass in range(0,npass):\n",
    "        \n",
    "        print '-----ipass='+str(ipass)+'----------'\n",
    "        random.shuffle(train)\n",
    "        this_train=train[0:ntrain]\n",
    "        this_test=train[ntrain:]\n",
    "\n",
    "        feat = Featurizer()\n",
    "    \n",
    "        labels = []\n",
    "        for line in this_train:\n",
    "            if not line[kTARGET_FIELD] in labels:\n",
    "                labels.append(line[kTARGET_FIELD])\n",
    "\n",
    "        print(\"Label set: %s\" % str(labels))\n",
    "        \n",
    "        x_train = feat.train_feature([[x[kTEXT_FIELD] for x in this_train], [x[kTROPE_FIELD] for x in this_train], [x[kPAGE_FIELD] for x in this_train]])\n",
    "        x_test = feat.test_feature([[x[kTEXT_FIELD] for x in this_test], [x[kTROPE_FIELD] for x in this_test], [x[kPAGE_FIELD] for x in this_test]])\n",
    "\n",
    "        y_train = array(list(labels.index(x[kTARGET_FIELD]) for x in this_train))\n",
    "\n",
    "        # Train classifier, using LogisticRegression\n",
    "        lr = SGDClassifier(loss='log', penalty='l2', shuffle=True)\n",
    "        lr.fit(x_train, y_train)\n",
    "        \n",
    "        predictions = lr.predict(x_test)\n",
    "        \n",
    "        #verify score\n",
    "        reference=array(list(labels.index(x[kTARGET_FIELD]) for x in this_test))\n",
    "        diff=np.sum(np.absolute(reference-predictions))\n",
    "        scores[ipass]=1.0-diff*1.0/ntest\n",
    "        print(scores[ipass])\n",
    "        \n",
    "    #print scores\n",
    "    print np.mean(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate frequency of some tags in a sentence\n",
    "class TagTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, examples):\n",
    "        # return self and nothing else \n",
    "        return self\n",
    "    \n",
    "    def transform(self, examples):\n",
    "        \n",
    "        import numpy as np \n",
    "        from scipy.sparse import csr_matrix\n",
    "        \n",
    "        # can add more tags, 'CD' number\n",
    "        tags = ['CD']\n",
    "        \n",
    "        # Initiaize matrix \n",
    "        X = np.zeros((len(examples), 1))\n",
    "        \n",
    "        # Loop over examples and count letters \n",
    "        for ii, x in enumerate(examples):\n",
    "            tag = nltk.pos_tag(nltk.word_tokenize(x))            \n",
    "            X[ii,0] = [t[1] for t in tag].count('CD') \n",
    "        print X\n",
    "        \n",
    "        # normalization for a feature\n",
    "        X = preprocessing.normalize(X, norm='l2')\n",
    "        return csr_matrix(X) \n",
    "\n",
    "# get name_entity\n",
    "def get_continuous_chunks(text):\n",
    "    chunked = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "    prev = None\n",
    "    continuous_chunk = []\n",
    "    num_name_entity = 0\n",
    "    current_chunk = []\n",
    "    for i in chunked:\n",
    "        if type(i) == Tree:\n",
    "            current_chunk.append(\" \".join([token for token, pos in i.leaves()]))\n",
    "        elif current_chunk:\n",
    "            named_entity = \" \".join(current_chunk)\n",
    "            if named_entity not in continuous_chunk:\n",
    "                num_name_entity += 1 \n",
    "                continuous_chunk.append(named_entity)\n",
    "                current_chunk = []\n",
    "            else:\n",
    "                num_name_entity += 1 \n",
    "                continue\n",
    "    return num_name_entity\n",
    "\n",
    "# get the frequency of name_entity in each sentence\n",
    "class NameTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, examples):\n",
    "        # return self and nothing else \n",
    "        return self\n",
    "    \n",
    "    def transform(self, examples):\n",
    "        \n",
    "        import numpy as np \n",
    "        from scipy.sparse import csr_matrix\n",
    "        \n",
    "        # Initiaize matrix \n",
    "        X = np.zeros((len(examples), 1))\n",
    "        \n",
    "        # Loop over examples and count letters \n",
    "        for ii, x in enumerate(examples):\n",
    "            X[ii,:] = get_continuous_chunks(x)\n",
    "            \n",
    "        return csr_matrix(X) \n",
    "\n",
    "# Additional, can also try topic model with LDA\n",
    "# https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
